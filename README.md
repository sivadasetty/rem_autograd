# rem_autograd
Relative entropy or KL divergence minimization using autograd tools.

Utils include:
1. Grad and Hessian calculations using [jax](https://github.com/google/jax).
2. Newton-Raphson optimization.
3. Adam or other optimization using [optax](https://github.com/google-deepmind/optax).
